# ğŸ“Š Spatio Benchmark System

A comprehensive, automated benchmark generation system that creates dynamic performance documentation for the SpatioLite project.

## ğŸ¯ Overview

The SpatioLite Benchmark System automatically:
- âœ… Runs comprehensive performance tests using Criterion.rs
- âœ… Generates formatted performance tables with system information
- âœ… Updates README.md with current benchmark results
- âœ… Provides multiple workflows for different use cases
- âœ… Creates sample data for testing and demonstrations

## ğŸš€ Quick Start

```bash
# Complete workflow: benchmark + README update
just bench

# Just update README with existing results
just bench-update

# Create sample results for testing
just bench-sample

# Get detailed help
just bench-help
```

## ğŸ“ System Architecture

```
scripts/
â”œâ”€â”€ run_benchmarks.sh          # ğŸ¯ Main benchmark orchestrator
â”œâ”€â”€ update_readme.sh           # ğŸ“ README content updater
â”œâ”€â”€ generate_benchmarks.rs     # ğŸ”§ Benchmark result parser
â”œâ”€â”€ create_sample_results.rs   # ğŸ“Š Sample data generator
â”œâ”€â”€ benchmark_help.sh          # â“ Comprehensive help system
â”œâ”€â”€ Cargo.toml                 # ğŸ“¦ Script dependencies
â””â”€â”€ README.md                  # ğŸ“š Detailed documentation
```

## ğŸ”„ Workflow Types

### 1. Complete Benchmark Run (Recommended)
```bash
./scripts/run_benchmarks.sh
```
**Duration**: 5-15 minutes
**Output**: Real performance data + updated README
**Use Case**: Before releases, after performance changes

### 2. Quick README Update
```bash
./scripts/update_readme.sh
```
**Duration**: Seconds
**Output**: Updated README with existing data
**Use Case**: Documentation updates, formatting changes

### 3. Sample Data Generation
```bash
cd scripts && cargo run --bin create_sample_results
```
**Duration**: Seconds
**Output**: Realistic sample benchmark data
**Use Case**: Testing, demos, when real benchmarks fail

## ğŸ“Š Benchmark Categories

| Category | Icon | Operations Tested | Key Metrics |
|----------|------|------------------|-------------|
| **Basic Operations** | ğŸ”§ | insert, get, batch | Ops/sec, latency |
| **Spatial Operations** | ğŸ“ | geohash, S2, points | Spatial throughput |
| **Trajectory Operations** | ğŸ“ˆ | trajectory CRUD | Time-series performance |
| **Concurrent Operations** | ğŸ§µ | multi-threaded ops | Concurrency efficiency |
| **High Throughput** | âš¡ | sustained operations | Peak performance |
| **Large Datasets** | ğŸ“Š | 1K-100K records | Scalability limits |
| **Persistence** | ğŸ’¾ | AOF writes/sync | Storage performance |
| **Spatial Indexing** | ğŸ—‚ï¸ | indexed vs linear | Index effectiveness |
| **TTL Operations** | â° | expiring data | TTL overhead |

## ğŸ“ˆ Generated Output

### Primary Files
- **`BENCHMARK_RESULTS.md`** - Complete performance report with system info
- **`benchmark_snippet.md`** - README-ready content with comment markers
- **`README.md`** - Automatically updated Performance section

### Sample Output Format
```markdown
| Operation Category | Test | Performance | Throughput |
|-------------------|------|-------------|------------|
| ğŸ”§ Basic Operations | Single Insert | 428 ns | 2.3M ops/sec |
| ğŸ“ Spatial Operations | Point Insert | 315 ns | 3.2M ops/sec |
| ğŸ§µ Concurrent Operations | Concurrent Inserts | 2.1 Î¼s | 46.5M ops/sec |
```

## ğŸ› ï¸ Technical Implementation

### README Integration
Uses HTML comment markers for automatic content replacement:
```markdown
<!-- BENCHMARK_RESULTS_START -->
... generated content replaces everything here ...
<!-- BENCHMARK_RESULTS_END -->
```

### Benchmark Parsing
- Parses `cargo bench` output using regex patterns
- Supports both standard and Criterion.rs output formats
- Extracts timing data and converts to standardized units
- Groups tests by category for organized presentation

### System Information
Automatically captures:
- CPU model and specifications
- Memory configuration
- Operating system details
- Timestamp and build information

## ğŸ›ï¸ Customization Guide

### Table Formatting
Edit `scripts/generate_benchmarks.rs`:

```rust
fn format_group_name(name: &str) -> String {
    match name {
        "basic_operations" => "ğŸ”§ Basic Operations",
        "your_category" => "ğŸ¯ Your Category",
        // Add new categories here
    }
}
```

### Throughput Calculations
```rust
fn calculate_throughput(test_name: &str, time_ns: f64) -> String {
    let ops_per_iter = if test_name.contains("batch_100") {
        100.0  // 100 operations per iteration
    } else {
        1.0    // Single operation per iteration
    };
    // Calculation logic...
}
```

### Category Organization
```rust
let group_order = vec![
    "basic_operations",
    "spatial_operations",
    "your_new_category",  // Add here
];
```

## ğŸ› Troubleshooting

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **Benchmarks don't run** | Error during `cargo bench` | Check benchmark code in `benches/` |
| **No output generated** | Empty results files | Use sample generator for testing |
| **README update fails** | Original content preserved | Verify comment markers exist |
| **System info missing** | Generic system description | Platform-specific commands needed |
| **Build failures** | Compilation errors | Check Rust version and dependencies |

### Debug Steps
```bash
# 1. Test basic benchmark functionality
cargo bench

# 2. Test benchmark parsing
cd scripts && cargo run --bin generate_benchmarks

# 3. Test README integration
./scripts/update_readme.sh

# 4. Rollback if needed
mv README.md.backup README.md
```

## ğŸ’¡ Best Practices

### For Accurate Results
- ğŸ”‡ **Quiet System**: Close unnecessary applications
- ğŸ”„ **Multiple Runs**: Verify consistency across runs
- âš¡ **Release Mode**: Always use optimized builds
- ğŸ“Š **Stable Environment**: Consistent hardware/software setup

### For Maintenance
- ğŸ“… **Regular Updates**: Run after significant changes
- ğŸ’¾ **Version Control**: Commit benchmark results with code
- ğŸ“ˆ **Trend Monitoring**: Track performance over time
- ğŸ¯ **Targeted Testing**: Focus on changed components

### For Development
- ğŸ§ª **Sample Data**: Use for workflow testing
- ğŸ”§ **Incremental**: Test individual components
- ğŸ“ **Documentation**: Update help when adding features
- ğŸ”„ **Automation**: Consider git hooks for consistency

## ğŸš€ Advanced Usage

### Git Integration
```bash
# Pre-commit hook for benchmark updates
echo './scripts/run_benchmarks.sh' > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

### Performance Analysis
```bash
# Compare benchmark results over time
git log --oneline --follow -- BENCHMARK_RESULTS.md

# Track specific performance metrics
grep "Single Insert" BENCHMARK_RESULTS.md
```

### CI/CD Integration
- **Manual Triggers Only**: Avoid automatic benchmark runs in CI
- **Artifact Storage**: Save benchmark results as build artifacts
- **Performance Gates**: Fail builds on significant regressions

## ğŸ“Š Performance Expectations

Based on the sample data, SpatioLite achieves:

- **ğŸš€ Basic Operations**: 2-3M ops/sec with sub-microsecond latency
- **ğŸ“ Spatial Inserts**: 2M+ ops/sec with automatic indexing
- **ğŸ” Spatial Queries**: Millisecond-range for complex searches
- **ğŸ§µ Concurrency**: Excellent scaling with minimal contention
- **ğŸ’¾ Persistence**: Fast AOF writes with configurable sync policies

## ğŸ”® Future Enhancements

### Planned Features
- [ ] Performance regression detection
- [ ] Benchmark result comparison tools
- [ ] JSON output format for tooling integration
- [ ] Custom benchmark configuration files
- [ ] Performance trend visualization

### Integration Opportunities
- [ ] Grafana dashboard integration
- [ ] Slack/Discord performance notifications
- [ ] Performance budgets and alerts
- [ ] Automated performance reports

## ğŸ“š Related Documentation

- **`scripts/README.md`** - Detailed script documentation
- **`benches/spatial_benchmarks.rs`** - Actual benchmark implementations
- **`README.md`** - Project documentation with current results
- **`PERFORMANCE.md`** - Static performance analysis (if exists)

## ğŸ‰ Success Metrics

The benchmark system is successful when:
- âœ… Benchmarks run reliably across different systems
- âœ… README stays current with minimal manual effort
- âœ… Performance regressions are caught early
- âœ… Contributors can easily update performance documentation
- âœ… Users have accurate performance expectations

---

**Built with â¤ï¸ for the SpatioLite project**

*For questions or improvements, see the individual script files or run `./scripts/benchmark_help.sh`*
