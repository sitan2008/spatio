# Spatio Scripts

This directory contains utility scripts for the Spatio project, focusing on benchmark generation and README maintenance.

## ğŸš€ Quick Start

```bash
# Complete benchmark workflow (recommended)
./run_benchmarks.sh

# Update README with existing results
./update_readme.sh

# Get help and detailed information
./benchmark_help.sh

# Create sample results for testing
cd scripts && cargo run --bin create_sample_results
```

## ğŸ“Š Benchmark System Overview

The Spatio benchmark system provides automated performance testing and documentation generation:

- **ğŸ”§ Automated Benchmarking**: Full test suite with Criterion.rs
- **ğŸ“ˆ Dynamic Results**: Real-time performance data generation
- **ğŸ“ README Integration**: Automatic documentation updates
- **ğŸ–¥ï¸ System-Aware**: Captures hardware and environment info
- **ğŸ¯ Multiple Workflows**: From quick updates to detailed analysis

## ğŸ› ï¸ Available Tools

### 1. Main Benchmark Runner (`run_benchmarks.sh`)
**Purpose**: Complete benchmark workflow from execution to README generation
**Usage**: `./run_benchmarks.sh`
**Output**:
- `BENCHMARK_RESULTS.md` - Full performance report
- `benchmark_snippet.md` - README-ready content

### 2. README Updater (`update_readme.sh`)
**Purpose**: Update README.md with benchmark results (existing or sample)
**Usage**: `./update_readme.sh`
**Features**:
- Automatic backup creation
- Marker-based content replacement
- Rollback instructions

### 3. Benchmark Generator (`generate_benchmarks.rs`)
**Purpose**: Parse cargo bench output and create formatted tables
**Usage**: `cargo run --bin generate_benchmarks`
**Features**:
- Multiple output formats
- System information capture
- Throughput calculations

### 4. Sample Results Creator (`create_sample_results.rs`)
**Purpose**: Generate realistic sample data for testing and demos
**Usage**: `cargo run --bin create_sample_results`
**Use Cases**:
- Testing documentation workflows
- Demo performance capabilities
- Fallback when real benchmarks fail

### 5. Help System (`benchmark_help.sh`)
**Purpose**: Comprehensive documentation and troubleshooting
**Usage**: `./benchmark_help.sh`
**Content**:
- Detailed workflow explanations
- Troubleshooting guides
- Customization instructions

## ğŸ“ˆ Benchmark Categories

| Category | Icon | Description | Examples |
|----------|------|-------------|----------|
| Basic Operations | ğŸ”§ | Core key-value operations | insert, get, batch |
| Spatial Operations | ğŸ“ | Geospatial indexing and queries | geohash, S2, points |
| Trajectory Operations | ğŸ“ˆ | Time-series spatial data | trajectory insert/query |
| Concurrent Operations | ğŸ§µ | Multi-threaded performance | concurrent inserts |
| High Throughput | âš¡ | Sustained operation rates | bulk operations |
| Large Datasets | ğŸ“Š | Big data performance | 1K-100K records |
| Persistence | ğŸ’¾ | Storage and sync operations | AOF writes |
| Spatial Indexing | ğŸ—‚ï¸ | Index performance comparison | indexed vs linear |
| TTL Operations | â° | Time-to-live functionality | expiring data |

## ğŸ”„ Workflows

### Complete Benchmark Update
```bash
./run_benchmarks.sh      # Run benchmarks + generate results
./update_readme.sh       # Update README automatically
git add README.md scripts/
git commit -m "Update benchmark results"
```

### Quick README Update (existing results)
```bash
./update_readme.sh       # Use existing benchmark data
git add README.md
git commit -m "Update performance documentation"
```

### Testing and Development
```bash
cd scripts
cargo run --bin create_sample_results    # Create sample data
cd ..
./update_readme.sh                      # Test README update
mv README.md.backup README.md           # Rollback if needed
```

### Manual Control
```bash
cd scripts && cargo build --release     # Build tools
cd .. && cargo bench                    # Run benchmarks
cd scripts && cargo run --bin generate_benchmarks  # Generate results
cd .. && ./update_readme.sh            # Update README
```

## ğŸ“ README Integration

The system uses HTML comments to mark the benchmark section:

```markdown
<!-- BENCHMARK_RESULTS_START -->
... benchmark content gets inserted here ...
<!-- BENCHMARK_RESULTS_END -->
```

**Benefits**:
- âœ… Preserves all other README content
- âœ… Automatic content replacement
- âœ… Version-controllable benchmark data
- âœ… Easy rollback with backup files

## ğŸ›ï¸ Customization

### Table Formatting
Edit `generate_benchmarks.rs`:
- `format_group_name()` - Category display names
- `format_test_name()` - Test name formatting
- `calculate_throughput()` - Throughput calculations
- `group_order` - Table organization

### System Information
Modify `get_system_info()` in `generate_benchmarks.rs`:
- Add new system metrics
- Platform-specific commands
- Custom environment details

### README Structure
Update comment markers in `README.md` to change integration points.

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| Benchmarks fail to run | Check `cargo bench` works manually |
| No output generated | Use `create_sample_results` for testing |
| README update fails | Verify comment markers exist |
| System info missing | Some commands are platform-specific |
| Build errors | Ensure correct directory and Rust version |

## ğŸ“‹ Requirements

- **Rust**: Latest stable toolchain
- **Platform**: Unix-like environment (macOS, Linux)
- **Dependencies**: No external crates required
- **Benchmarks**: Working `benches/` directory with Criterion.rs

## ğŸ’¡ Best Practices

- **ğŸ”‡ Quiet System**: Close other applications during benchmarking
- **ğŸ”„ Multiple Runs**: Check consistency across benchmark runs
- **ğŸ“Š Regular Updates**: Update benchmarks after significant changes
- **ğŸ’¾ Version Control**: Commit benchmark results with code changes
- **ğŸ“ˆ Performance Tracking**: Monitor performance trends over time

## ğŸš€ Advanced Usage

### Git Integration
```bash
# Pre-push hook for automatic benchmark updates
echo './scripts/run_benchmarks.sh && ./scripts/update_readme.sh' > .git/hooks/pre-push
chmod +x .git/hooks/pre-push
```

### CI/CD Integration
- Manual benchmark triggers only (not automatic)
- Store results as artifacts
- Compare performance across branches

### Performance Analysis
- Use `BENCHMARK_RESULTS.md` for detailed analysis
- Track performance regressions
- Identify optimization opportunities

---

**ğŸ“š For more information, run `./benchmark_help.sh` or check the individual script files.**
